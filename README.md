# DL-notebook

在深度学习中，局部极小值和鞍点是优化过程中常见的两种现象，它们与神经网络的损失函数有关。

## 局部极小值和鞍点

### 1. 局部极小值（Local Minimum）
局部极小值是指在某个区域内，损失函数的值比该区域内的其他点都要小。换句话说，在这个点附近，任何微小的参数变化都会导致损失函数的值增加。局部极小值不是全局最优解，但它在一定范围内是最优的。

例子：
假设你在一个山谷里，山谷的底部就是局部极小值。你在山谷的底部，往任何方向移动都会使你上升，这意味着你的“高度”（即损失函数的值）增加。

### 2. 鞍点（Saddle Point）
鞍点是损失函数的一个特殊点，在这个点上，损失函数的梯度（即变化率）为零，但是它既不是局部极小值也不是局部极大值。鞍点在某些方向上表现为极小值，而在其他方向上表现为极大值。

例子：
假设你骑在马鞍的中央位置。沿着马背的方向，你向前或向后移动，都会让你上升（类似于局部极大值）；但如果你往马鞍两侧的方向移动，你会下降（类似于局部极小值）。因此，这个点既不是纯粹的最低点（局部极小值），也不是最高点（局部极大值）。

### 3. 优化中的挑战
在神经网络的训练过程中，优化算法（如梯度下降）会在损失函数的表面上“爬坡”或“下坡”，试图找到最低的点（最小化损失函数）。然而，由于高维空间的复杂性，梯度下降可能会停留在鞍点上，因为在这个点上，损失函数的梯度为零，看起来像是“平坦”的。

局部极小值和鞍点都可能导致优化停滞，但它们的处理方式不同：

局部极小值：如果梯度下降算法停在局部极小值上，这意味着在这个点附近没有方向可以进一步减少损失，可能无法达到全局最优解。
鞍点：由于在鞍点的一些方向上损失仍然可以下降，所以找到这些方向后可以继续优化。

### 4. 鞍点的逃离

在实践中，深度学习模型往往遇到的更多是鞍点而非局部极小值。因为高维空间中的局部极小值通常较少，而鞍点非常多。通过合适的算法调整（例如动量法、学习率调度、自适应优化算法等），可以帮助模型逃离鞍点，继续朝向更低的损失点移动。


